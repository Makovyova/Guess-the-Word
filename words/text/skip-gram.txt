#Generate Data
def tokenize(text):
    pattern = re.compile(r'[A-Za-z]+[\w^\']*|[\w^\']*[A-Za-z]+[\w^\']*')
    return pattern.findall(text.lower())
tokens = tokenize("Hello from the other side")
print(tokens)
#['hello', 'from', 'the', 'other', 'side']

id_to_word = {i:x for (i, x) in enumerate(tokens)}
word_to_id = {x:i for (i, x) in enumerate(tokens)}
print(word_to_id)
print(id_to_word)
#{'hello': 0, 'from': 1, 'the': 2, 'other': 3, 'side': 4}
#{0: 'hello', 1: 'from', 2: 'the', 3: 'other', 4: 'side'}

def generate_training_data(tokens, word_to_id, window_size):
    X, Y = [], []

    for i in range(len(tokens)):
        nbr_inds = list(range(max(0, i - window_size), i)) + \
                   list(range(i + 1, min(len(tokens), i + window_size + 1)))
        for j in nbr_inds:
            X.append(word_to_id[tokens[i]])
            Y.append(word_to_id[tokens[j]])
            
    return np.array(X), np.array(Y)

def expand_dims(x, y):
    x = np.expand_dims(x, axis=0)
    y = np.expand_dims(y, axis=0)
    return x, y

x, y = generate_training_data(tokens, word_to_id, 3)
x, y = expand_dims(x, y)
# generated training data
x, y
#(array([[0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4]]),
# array([[1, 2, 3, 0, 2, 3, 4, 0, 1, 3, 4, 0, 1, 2, 4, 1, 2, 3]]))

#Forward Propagation
def init_parameters(vocab_size, emb_size):
    wrd_emb = np.random.randn(vocab_size, emb_size) * 0.01
    w = np.random.randn(vocab_size, emb_size) * 0.01
    
    return wrd_emb, w
def softmax(z):
    return np.divide(np.exp(z), np.sum(np.exp(z), axis=0, keepdims=True) + 0.001)
def forward(inds, params):
    wrd_emb, w = params
    word_vec = wrd_emb[inds.flatten(), :].T
    z = np.dot(w, word_vec)
    out = softmax(z)
    
    cache = inds, word_vec, w, z
    
    return out, cache

#Cost Function
def cross_entropy(y, y_hat):
    m = y.shape[1]
    cost = -(1 / m) * np.sum(np.sum(y_hat * np.log(y + 0.001), axis=0, keepdims=True), axis=1)
    return cost

#Backward Propagation
def dsoftmax(y, out):
    dl_dz = out - y
    
    return dl_dz
def backward(y, out, cache):
    inds, word_vec, w, z = cache
    wrd_emb, w = params
    
    dl_dz = dsoftmax(y, out)
    # deviding by the word_vec length to find the average
    dl_dw = (1/word_vec.shape[1]) * np.dot(dl_dz, word_vec.T)
    dl_dword_vec = np.dot(w.T, dl_dz)
    
    return dl_dz, dl_dw, dl_dword_vec
def update(params, cache, grads, lr=0.03):
    inds, word_vec, w, z = cache
    wrd_emb, w = params
    dl_dz, dl_dw, dl_dword_vec = grads
    
    wrd_emb[inds.flatten(), :] -= dl_dword_vec.T * lr
    w -= dl_dw * lr
    
    return wrd_emb, w

#Training
vocab_size = len(id_to_word)

m = y.shape[1]
y_one_hot = np.zeros((vocab_size, m))
y_one_hot[y.flatten(), np.arange(m)] = 1

y = y_one_hot

batch_size=256
embed_size = 50

params = init_parameters(vocab_size, 50)

costs = []

for epoch in range(5000):
    epoch_cost = 0
    
    batch_inds = list(range(0, x.shape[1], batch_size))
    np.random.shuffle(batch_inds)
    
    for i in batch_inds:
        x_batch = x[:, i:i+batch_size]
        y_batch = y[:, i:i+batch_size]
        
        pred, cache = forward(x_batch, params)
        grads = backward(y_batch, pred, cache)
        params = update(params, cache, grads, 0.03)
        cost = cross_entropy(pred, y_batch)
        
        epoch_cost += np.squeeze(cost)
        
    costs.append(epoch_cost)
    
    if(epoch % 250 == 0):
        print("Cost after epoch {}: {}".format(epoch, epoch_cost))

'''
Cost after epoch 0: 1.6047175984341244
Cost after epoch 250: 1.5571992410033877
Cost after epoch 500: 1.5160138801755096
Cost after epoch 750: 1.5013244699272772
Cost after epoch 1000: 1.5033310889760447
Cost after epoch 1250: 1.509646557301732
Cost after epoch 1500: 1.5101079508309503
Cost after epoch 1750: 1.5111470208621518
Cost after epoch 2000: 1.516343929781987
Cost after epoch 2250: 1.522499350112024
Cost after epoch 2500: 1.5269100667088928
Cost after epoch 2750: 1.5291433717507568
Cost after epoch 3000: 1.5296328050180197
Cost after epoch 3250: 1.5288034717180867
Cost after epoch 3500: 1.52695010008721
Cost after epoch 3750: 1.5245662893667156
Cost after epoch 4000: 1.5225617388065675
Cost after epoch 4250: 1.521743112089992
Cost after epoch 4500: 1.5221533192798382
Cost after epoch 4750: 1.523232604416357
'''
#Test
x_test = np.arange(vocab_size)
x_test = np.expand_dims(x_test, axis=0)
softmax_test, _ = forward(x_test, params)
top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]
for input_ind in range(vocab_size):
    input_word = id_to_word[input_ind]
    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]
    print("{}'s skip-grams: {}".format(input_word, output_words))
'''
hello's skip-grams: ['other', 'from', 'the', 'side']
from's skip-grams: ['side', 'hello', 'the', 'other']
the's skip-grams: ['side', 'hello', 'from', 'other']
other's skip-grams: ['side', 'hello', 'from', 'the']
side's skip-grams: ['other', 'from', 'the', 'side']
'''